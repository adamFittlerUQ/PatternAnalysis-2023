# Brain Image Generation
The purpose of this report is to use the stylegan2 deep learning model with the OASIS brain dataset in order to produce realistic 
brain scan images. The StyleGAN2 model was chosen over other models generative models such as the DCGAN and VQVAE as the StyleGAN2 model is renound for its high-quality image generation compared to other models. Furthermore, the style vector allows for fine-grained control over the generated images' styles and features, making it ideal for creating a wide variety of outputs. 

Generating brain images using StyleGAN2 holds significant importance in various industries, primarily in healthcare and neuroscience. It offers a range of advantages, such as data augmentation, privacy preservation, and enhanced anomaly detection. By creating realistic synthetic brain images, it expands the availability of diverse training data for machine learning models, leading to more accurate diagnostic tools and predictive algorithms. Additionally, it addresses privacy concerns by enabling research and development without compromising patient data.

Furthermore, StyleGAN2-generated brain images find applications in education and training, helping medical professionals better understand complex conditions and treatments. They serve as valuable educational tools for medical students and professionals. These synthetic images also facilitate research into neurological disorders, enabling scientists to simulate different conditions and diseases, leading to innovative treatments and interventions.

In terms of cost-effectiveness, generating synthetic brain images reduces the need for expensive data acquisition and medical equipment, making it an efficient option for testing algorithms and technologies. Additionally, it supports personalized medicine by allowing customization and simulation of individual patient profiles. Overall, StyleGAN2's role in generating brain images has the potential to revolutionize medical imaging, research, and healthcare, offering a versatile and ethical approach to data generation, analysis, and application development.


# Project Structure
- `modules.py`: The Generator and Discriminator networks as well as the layers and block that make them up. 
- `dataset.py`: Data loader and preprocessing for the OASIS brain dataset, performing augmentation.
- `train.py`: Training process of for the generator and discriminator networks and the loss calculations and optimization.
- `predict.py`: Use of the model and display of the progress of the loss for both networks and generated images. 


# How to Use
In order to run this model on the OASIS dataset and see the generated images, you can run the predict.py file.
This file imports the necessary dataset, modules and training process from the dataset.py, modules.py and train.py files respectively.
It trains the model on the OASIS dataset and then provides the loss for each epoch as well as the generated images. After the training
is complete it plots a graph of the losses of the generator and discriminator networks over time to show that they begin to converge. 


# Understanding the Model 
## Introduction to Stylegan
The stylegan2 architecture build upon that of stylegan and the basic gan architecture. It made up of a generative model and discrimintative model.
The generator will create images and discriminator will decide whether the image is from the real data set or if it has been created by the generator network. Both, of the networks work with and against each other, the generator trying to fool the discriminator and discriminator trying not to be fooled.  The end goal being the generation of realistic images close to that of the real provided dataset.  

The original stylegan model creared by Nvidia in December of 2018, built upon this basic gan architecture aiming to not only improve the quality of the resulting images but also providing more control over the specific styles of the images. The stylegan model introduced the style vector and noise components into the model, replacing the latent vector used in traditional gan architectures.  It also employs the adaptive instance normalisation (AdaIN) layer. Instance normalisation is a technique used to normalise the activations of each feature map independently, making the mean activation and the startdard deviation one for each feature map.  The stylegan's AdaIn layer takes this a step further by allowing the network to adapt the mean and standard deviation of feature maps for each layer dynamically. This adaptation is controlled by the style vector w, which is generated by the mapping network. This is what allows for the styles to be adjusted post training.

Stylegan also implements a more advanced mapping network that is responaible for mapping a random noise vector to the style vector w.  This mapping network contains multiple fully connected or linear layers that are connected through non-linear activation functions such as Leaky ReLU. During the training process the mapping network learns to map the noise vector into meaningful latent codes that can be used to effectively control the generators output.  

## Inprovements in Stylegan2
The Stylegan2 model architecture expands upon that of the original Stylegan architecture and aims to solve the main problem of that architecture that being the image artifacts that stylegan introduced.  The original Stylegan model was notirious for leaving small anomolies in the images that would allow for images that had been generated by the Stylegan network to be easily identifiable. 

Stylegan2 restricts the use of the adaptive instance noramlisation, get away from progressive growing to resolve the artifacts introduced in Stylegan, and introduces a perceptual path length normalisation term in it's loss function to improve the latent space interpolation ability.  


# Model Architecture Implementation
The implementation of the Stylegan2 model used in this report is taken directly from the original paper, however it has been simplified drastically and made to be more compact and readable due to the simpler nature of this specific generation task. We will cover all of the important layers explaining their usages and the theory behind them.  These networks and layers can be seen in the modules.py file. 

## Noise Mapping Network

## Generator Network
### Generator

### Generator Block

### Style Block

### ToRGB

### Convolution Weight Modulation/Demodulation

## Discriminator
### Discriminator Block

### Equalised Linear Layer

## Equalised Convolutional Layer

### Equalised Weight 

## Perceptual Path Normalisation
### PathLengthPenalty


## Utilities 
### Gradient Penalty
### Get W
### Get Noise
### Generate Examples

# Training Process

# Results
After completing the stylegan2's training on the OASIS brain dataset for 10 epochs, the following realisitc images were generated by the trained generator model. In order to display the use of the style vector in the stylegan model's architecture, the images were generated passing different style vectors to the generator in order to display how the stylegan model once trained is able to be styled and shaped. The results have also been compared to that of the original images in the OASIS dataset in order to judge how realsitic the final generators images are.

---------Images Here

---------Continue Here

A graph of the loss for both the generator and discriminator networks of the models architecture can be seen below. It shows that the model training perfomed relatively as expected as over time the loss of the generator model decreased and towards the end the discriminator became a little worse as do the to generator network's images becoming more and more realistic. This is exactly what we wanted to see for this model!

---------Images Here 


## Acknowledgments (Todo)
The structure of the models used in this report were created using the styleGAN2 reasearch paper.  


## Todo List
- Set RBG to all same value to get a grayscale image
- Check if style vector w manipulation shown is good enough with tutor

