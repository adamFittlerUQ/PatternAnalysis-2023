{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from math import log2, sqrt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET                 = \"./OASIS\"\n",
    "DEVICE                  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPOCHS                  = 300\n",
    "LEARNING_RATE           = 1e-3\n",
    "BATCH_SIZE              = 32\n",
    "LOG_RESOLUTION          = 7 #for 128*128\n",
    "Z_DIM                   = 256\n",
    "W_DIM                   = 256\n",
    "LAMBDA_GP               = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader():\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((2 ** LOG_RESOLUTION, 2 ** LOG_RESOLUTION)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.Normalize(\n",
    "                [0.5, 0.5, 0.5],\n",
    "                [0.5, 0.5, 0.5],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    dataset = datasets.ImageFolder(root=DATASET, transform=transform)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, z_dim, w_dim):\n",
    "        super().__init__()\n",
    "        self.mapping = nn.Sequential(\n",
    "            EqualizedLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            EqualizedLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            EqualizedLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            EqualizedLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            EqualizedLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            EqualizedLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            EqualizedLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            EqualizedLinear(z_dim, w_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + 1e-8)  # for PixelNorm \n",
    "        return self.mapping(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, log_resolution, W_DIM, n_features = 32, max_features = 256):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 2, -1, -1)]\n",
    "        self.n_blocks = len(features)\n",
    "\n",
    "        self.initial_constant = nn.Parameter(torch.randn((1, features[0], 4, 4)))\n",
    "\n",
    "        self.style_block = StyleBlock(W_DIM, features[0], features[0])\n",
    "        self.to_rgb = ToRGB(W_DIM, features[0])\n",
    "\n",
    "        blocks = [GeneratorBlock(W_DIM, features[i - 1], features[i]) for i in range(1, self.n_blocks)]\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, w, input_noise):\n",
    "\n",
    "        batch_size = w.shape[1]\n",
    "\n",
    "        x = self.initial_constant.expand(batch_size, -1, -1, -1)\n",
    "        x = self.style_block(x, w[0], input_noise[0][1])\n",
    "        rgb = self.to_rgb(x, w[0])\n",
    "\n",
    "        for i in range(1, self.n_blocks):\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"bilinear\")\n",
    "            x, rgb_new = self.blocks[i - 1](x, w[i], input_noise[i])\n",
    "            rgb = F.interpolate(rgb, scale_factor=2, mode=\"bilinear\") + rgb_new\n",
    "\n",
    "        return torch.tanh(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, W_DIM, in_features, out_features):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.style_block1 = StyleBlock(W_DIM, in_features, out_features)\n",
    "        self.style_block2 = StyleBlock(W_DIM, out_features, out_features)\n",
    "\n",
    "        self.to_rgb = ToRGB(W_DIM, out_features)\n",
    "\n",
    "    def forward(self, x, w, noise):\n",
    "\n",
    "        x = self.style_block1(x, w, noise[0])\n",
    "        x = self.style_block2(x, w, noise[1])\n",
    "\n",
    "        rgb = self.to_rgb(x, w)\n",
    "\n",
    "        return x, rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, W_DIM, in_features, out_features):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.to_style = EqualizedLinear(W_DIM, in_features, bias=1.0)\n",
    "        self.conv = Conv2dWeightModulate(in_features, out_features, kernel_size=3)\n",
    "        self.scale_noise = nn.Parameter(torch.zeros(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "\n",
    "    def forward(self, x, w, noise):\n",
    "\n",
    "        s = self.to_style(w)\n",
    "        x = self.conv(x, s)\n",
    "        if noise is not None:\n",
    "            x = x + self.scale_noise[None, :, None, None] * noise\n",
    "        return self.activation(x + self.bias[None, :, None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToRGB(nn.Module):\n",
    "\n",
    "    def __init__(self, W_DIM, features):\n",
    "\n",
    "        super().__init__()\n",
    "        self.to_style = EqualizedLinear(W_DIM, features, bias=1.0)\n",
    "\n",
    "        self.conv = Conv2dWeightModulate(features, 3, kernel_size=1, demodulate=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(3))\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "\n",
    "        style = self.to_style(w)\n",
    "        x = self.conv(x, style)\n",
    "        return self.activation(x + self.bias[None, :, None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dWeightModulate(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, kernel_size,\n",
    "                 demodulate = True, eps = 1e-8):\n",
    "\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.demodulate = demodulate\n",
    "        self.padding = (kernel_size - 1) // 2\n",
    "\n",
    "        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, s):\n",
    "\n",
    "        b, _, h, w = x.shape\n",
    "\n",
    "        s = s[:, None, :, None, None]\n",
    "        weights = self.weight()[None, :, :, :, :]\n",
    "        weights = weights * s\n",
    "\n",
    "        if self.demodulate:\n",
    "            sigma_inv = torch.rsqrt((weights ** 2).sum(dim=(2, 3, 4), keepdim=True) + self.eps)\n",
    "            weights = weights * sigma_inv\n",
    "\n",
    "        x = x.reshape(1, -1, h, w)\n",
    "\n",
    "        _, _, *ws = weights.shape\n",
    "        weights = weights.reshape(b * self.out_features, *ws)\n",
    "\n",
    "        x = F.conv2d(x, weights, padding=self.padding, groups=b)\n",
    "\n",
    "        return x.reshape(-1, self.out_features, h, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, log_resolution, n_features = 64, max_features = 256):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 1)]\n",
    "\n",
    "        self.from_rgb = nn.Sequential(\n",
    "            EqualizedConv2d(3, n_features, 1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        n_blocks = len(features) - 1\n",
    "        blocks = [DiscriminatorBlock(features[i], features[i + 1]) for i in range(n_blocks)]\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "        final_features = features[-1] + 1\n",
    "        self.conv = EqualizedConv2d(final_features, final_features, 3)\n",
    "        self.final = EqualizedLinear(2 * 2 * final_features, 1)\n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "        batch_statistics = (\n",
    "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        )\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.from_rgb(x)\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        x = self.minibatch_std(x)\n",
    "        x = self.conv(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.residual = nn.Sequential(nn.AvgPool2d(kernel_size=2, stride=2), # down sampling using avg pool\n",
    "                                      EqualizedConv2d(in_features, out_features, kernel_size=1))\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            EqualizedConv2d(in_features, in_features, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            EqualizedConv2d(in_features, out_features, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "\n",
    "        self.down_sample = nn.AvgPool2d(\n",
    "            kernel_size=2, stride=2\n",
    "        )  # down sampling using avg pool\n",
    "\n",
    "        self.scale = 1 / sqrt(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual(x)\n",
    "\n",
    "        x = self.block(x)\n",
    "        x = self.down_sample(x)\n",
    "\n",
    "        return (x + residual) * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualizedLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias = 0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.weight = EqualizedWeight([out_features, in_features])\n",
    "        self.bias = nn.Parameter(torch.ones(out_features) * bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return F.linear(x, self.weight(), bias=self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualizedConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features,\n",
    "                 kernel_size, padding = 0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.padding = padding\n",
    "        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\n",
    "        self.bias = nn.Parameter(torch.ones(out_features))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return F.conv2d(x, self.weight(), bias=self.bias, padding=self.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualizedWeight(nn.Module):\n",
    "\n",
    "    def __init__(self, shape):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.c = 1 / sqrt(np.prod(shape[1:]))\n",
    "        self.weight = nn.Parameter(torch.randn(shape))\n",
    "\n",
    "    def forward(self):\n",
    "        return self.weight * self.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathLengthPenalty(nn.Module):\n",
    "\n",
    "    def __init__(self, beta):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.beta = beta\n",
    "        self.steps = nn.Parameter(torch.tensor(0.), requires_grad=False)\n",
    "\n",
    "        self.exp_sum_a = nn.Parameter(torch.tensor(0.), requires_grad=False)\n",
    "\n",
    "    def forward(self, w, x):\n",
    "\n",
    "        device = x.device\n",
    "        image_size = x.shape[2] * x.shape[3]\n",
    "        y = torch.randn(x.shape, device=device)\n",
    "\n",
    "        output = (x * y).sum() / sqrt(image_size)\n",
    "        sqrt(image_size)\n",
    "\n",
    "        gradients, *_ = torch.autograd.grad(outputs=output,\n",
    "                                            inputs=w,\n",
    "                                            grad_outputs=torch.ones(output.shape, device=device),\n",
    "                                            create_graph=True)\n",
    "\n",
    "        norm = (gradients ** 2).sum(dim=2).mean(dim=1).sqrt()\n",
    "\n",
    "        if self.steps > 0:\n",
    "\n",
    "            a = self.exp_sum_a / (1 - self.beta ** self.steps)\n",
    "\n",
    "            loss = torch.mean((norm - a) ** 2)\n",
    "        else:\n",
    "            loss = norm.new_tensor(0)\n",
    "\n",
    "        mean = norm.mean().detach()\n",
    "        self.exp_sum_a.mul_(self.beta).add_(mean, alpha=1 - self.beta)\n",
    "        self.steps.add_(1.)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake,device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images)\n",
    " \n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w(batch_size):\n",
    "\n",
    "    z = torch.randn(batch_size, W_DIM).to(DEVICE)\n",
    "    w = mapping_network(z)\n",
    "    return w[None, :, :].expand(LOG_RESOLUTION, -1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(batch_size):\n",
    "    \n",
    "        noise = []\n",
    "        resolution = 4\n",
    "\n",
    "        for i in range(LOG_RESOLUTION):\n",
    "            if i == 0:\n",
    "                n1 = None\n",
    "            else:\n",
    "                n1 = torch.randn(batch_size, 1, resolution, resolution, device=DEVICE)\n",
    "            n2 = torch.randn(batch_size, 1, resolution, resolution, device=DEVICE)\n",
    "\n",
    "            noise.append((n1, n2))\n",
    "\n",
    "            resolution *= 2\n",
    "\n",
    "        return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(gen, epoch, n=100):\n",
    "    \n",
    "    gen.eval()\n",
    "    alpha = 1.0\n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            w     = get_w(1)\n",
    "            noise = get_noise(1)\n",
    "            img = gen(w, noise)\n",
    "            if not os.path.exists(f'saved_examples/epoch{epoch}'):\n",
    "                os.makedirs(f'saved_examples/epoch{epoch}')\n",
    "            save_image(img*0.5+0.5, f\"saved_examples/epoch{epoch}/img_{i}.png\")\n",
    "\n",
    "    gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    critic,\n",
    "    gen,\n",
    "    path_length_penalty,\n",
    "    loader,\n",
    "    opt_critic,\n",
    "    opt_gen,\n",
    "    opt_mapping_network,\n",
    "):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for batch_idx, (real, _) in enumerate(loop):\n",
    "        real = real.to(DEVICE)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        w     = get_w(cur_batch_size)\n",
    "        noise = get_noise(cur_batch_size)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake = gen(w, noise)\n",
    "            critic_fake = critic(fake.detach())\n",
    "            \n",
    "            critic_real = critic(real)\n",
    "            gp = gradient_penalty(critic, real, fake, device=DEVICE)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "                + LAMBDA_GP * gp\n",
    "                + (0.001 * torch.mean(critic_real ** 2))\n",
    "            )\n",
    "\n",
    "        critic.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        opt_critic.step()\n",
    "\n",
    "        gen_fake = critic(fake)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "\n",
    "        if batch_idx % 16 == 0:\n",
    "            plp = path_length_penalty(w, fake)\n",
    "            if not torch.isnan(plp):\n",
    "                loss_gen = loss_gen + plp\n",
    "\n",
    "        mapping_network.zero_grad()\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        opt_mapping_network.step()\n",
    "\n",
    "        loop.set_postfix(\n",
    "            gp=gp.item(),\n",
    "            loss_critic=loss_critic.item(),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MappingNetwork(\n",
       "  (mapping): Sequential(\n",
       "    (0): EqualizedLinear(\n",
       "      (weight): EqualizedWeight()\n",
       "    )\n",
       "    (1): ReLU()\n",
       "    (2): EqualizedLinear(\n",
       "      (weight): EqualizedWeight()\n",
       "    )\n",
       "    (3): ReLU()\n",
       "    (4): EqualizedLinear(\n",
       "      (weight): EqualizedWeight()\n",
       "    )\n",
       "    (5): ReLU()\n",
       "    (6): EqualizedLinear(\n",
       "      (weight): EqualizedWeight()\n",
       "    )\n",
       "    (7): ReLU()\n",
       "    (8): EqualizedLinear(\n",
       "      (weight): EqualizedWeight()\n",
       "    )\n",
       "    (9): ReLU()\n",
       "    (10): EqualizedLinear(\n",
       "      (weight): EqualizedWeight()\n",
       "    )\n",
       "    (11): ReLU()\n",
       "    (12): EqualizedLinear(\n",
       "      (weight): EqualizedWeight()\n",
       "    )\n",
       "    (13): ReLU()\n",
       "    (14): EqualizedLinear(\n",
       "      (weight): EqualizedWeight()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader              = get_loader()\n",
    "\n",
    "gen                 = Generator(LOG_RESOLUTION, W_DIM).to(DEVICE)\n",
    "critic              = Discriminator(LOG_RESOLUTION).to(DEVICE)\n",
    "mapping_network     = MappingNetwork(Z_DIM, W_DIM).to(DEVICE)\n",
    "path_length_penalty = PathLengthPenalty(0.99).to(DEVICE)\n",
    "\n",
    "opt_gen             = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
    "opt_critic          = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
    "opt_mapping_network = optim.Adam(mapping_network.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
    "\n",
    "gen.train()\n",
    "critic.train()\n",
    "mapping_network.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/708 [00:00<?, ?it/s]/home/adam/miniconda3/envs/stylegan/lib/python3.11/site-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "  0%|          | 2/708 [02:23<14:06:50, 71.97s/it, gp=0.972, loss_critic=8.48]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m loader \u001b[39m=\u001b[39m get_loader()  \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     train_fn(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         critic,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         gen,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         path_length_penalty,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         loader,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         opt_critic,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         opt_gen,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         opt_mapping_network,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \tgenerate_examples(gen, epoch)\n",
      "\u001b[1;32m/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     loss_critic \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m         \u001b[39m-\u001b[39m(torch\u001b[39m.\u001b[39mmean(critic_real) \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mmean(critic_fake))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m         \u001b[39m+\u001b[39m LAMBDA_GP \u001b[39m*\u001b[39m gp\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m         \u001b[39m+\u001b[39m (\u001b[39m0.001\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mmean(critic_real \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m critic\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m loss_critic\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m opt_critic\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m gen_fake \u001b[39m=\u001b[39m critic(fake)\n",
      "File \u001b[0;32m~/miniconda3/envs/stylegan/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/stylegan/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loader = get_loader()  \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_fn(\n",
    "        critic,\n",
    "        gen,\n",
    "        path_length_penalty,\n",
    "        loader,\n",
    "        opt_critic,\n",
    "        opt_gen,\n",
    "        opt_mapping_network,\n",
    "    )\n",
    "    if epoch % 50 == 0:\n",
    "    \tgenerate_examples(gen, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
