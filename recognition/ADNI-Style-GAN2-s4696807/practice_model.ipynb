{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from math import log2, sqrt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET                 = \"./AD_NC\"\n",
    "DEVICE                  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPOCHS                  = 300\n",
    "LEARNING_RATE           = 1e-3\n",
    "BATCH_SIZE              = 32\n",
    "LOG_RESOLUTION          = 7 #for 128*128\n",
    "Z_DIM                   = 256\n",
    "W_DIM                   = 256\n",
    "LAMBDA_GP               = 10\n",
    "WORKERS                 = 2         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADNIDataset(Dataset):\n",
    "    def __init__(self, root, split, transform=None):\n",
    "        self.root = root\n",
    "        self.split = split  # 'train' or 'test'\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Define the classes (AD and NC)\n",
    "        self.classes = ['AD', 'NC']\n",
    "\n",
    "        # Load image paths and labels\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root, split, class_name)\n",
    "            class_label = self.classes.index(class_name)\n",
    "            image_files = os.listdir(class_dir)\n",
    "            for image_file in image_files:\n",
    "                self.image_paths.append(os.path.join(class_dir, image_file))\n",
    "                self.labels.append(class_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('L')  # Convert to grayscale (1 channel)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader():\n",
    "    \n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((2 ** LOG_RESOLUTION, 2 ** LOG_RESOLUTION)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.Grayscale(num_output_channels=1),  # Ensure images are read as grayscale\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,)),  # Mean and std for single-channel images\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_dataset = ADNIDataset(root=DATASET, split='train', transform=transform)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS)\n",
    "    \n",
    "    \n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, z_dim, w_dim):\n",
    "        super().__init__()\n",
    "        self.mapping = nn.Sequential(\n",
    "            EqualizedLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            EqualizedLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            EqualizedLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            EqualizedLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            EqualizedLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            EqualizedLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            EqualizedLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            EqualizedLinear(z_dim, w_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + 1e-8)  # for PixelNorm \n",
    "        return self.mapping(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, log_resolution, W_DIM, n_features=32, max_features=256):\n",
    "        super().__init__()\n",
    "\n",
    "        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 2, -1, -1)]\n",
    "        self.n_blocks = len(features)\n",
    "\n",
    "        # Adjust the number of input channels from 3 to 1\n",
    "        self.initial_constant = nn.Parameter(torch.randn((1, 1, 4, 4)))\n",
    "\n",
    "        self.style_block = StyleBlock(W_DIM, features[0], features[0])\n",
    "        # Modify the ToRGB layer to output a single-channel image\n",
    "        self.to_rgb = ToRGB(W_DIM, features[0], out_channels=1)\n",
    "\n",
    "        blocks = [GeneratorBlock(W_DIM, features[i - 1], features[i]) for i in range(1, self.n_blocks)]\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, w, input_noise):\n",
    "\n",
    "        batch_size = w.shape[1]\n",
    "\n",
    "        x = self.initial_constant.expand(batch_size, -1, -1, -1)\n",
    "        x = self.style_block(x, w[0], input_noise[0][1])\n",
    "        rgb = self.to_rgb(x, w[0])\n",
    "\n",
    "        for i in range(1, self.n_blocks):\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"bilinear\")\n",
    "            x, rgb_new = self.blocks[i - 1](x, w[i], input_noise[i])\n",
    "            rgb = F.interpolate(rgb, scale_factor=2, mode=\"bilinear\") + rgb_new\n",
    "\n",
    "        return torch.tanh(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, W_DIM, in_features, out_features):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.style_block1 = StyleBlock(W_DIM, in_features, out_features)\n",
    "        self.style_block2 = StyleBlock(W_DIM, out_features, out_features)\n",
    "\n",
    "        self.to_rgb = ToRGB(W_DIM, out_features)\n",
    "\n",
    "    def forward(self, x, w, noise):\n",
    "\n",
    "        x = self.style_block1(x, w, noise[0])\n",
    "        x = self.style_block2(x, w, noise[1])\n",
    "\n",
    "        rgb = self.to_rgb(x, w)\n",
    "\n",
    "        return x, rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, W_DIM, in_features, out_features):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.to_style = EqualizedLinear(W_DIM, in_features, bias=1.0)\n",
    "        self.conv = Conv2dWeightModulate(in_features, out_features, kernel_size=3)\n",
    "        self.scale_noise = nn.Parameter(torch.zeros(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "\n",
    "    def forward(self, x, w, noise):\n",
    "\n",
    "        s = self.to_style(w)\n",
    "        x = self.conv(x, s)\n",
    "        if noise is not None:\n",
    "            x = x + self.scale_noise[None, :, None, None] * noise\n",
    "        return self.activation(x + self.bias[None, :, None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToRGB(nn.Module):\n",
    "    def __init__(self, W_DIM, features, out_channels=3):\n",
    "        super().__init__()\n",
    "        self.to_style = EqualizedLinear(W_DIM, features, bias=1.0)\n",
    "\n",
    "        # Modify the Conv2dWeightModulate layer to output a single-channel image\n",
    "        self.conv = Conv2dWeightModulate(features, out_channels, kernel_size=1, demodulate=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "        self.activation = nn.LeakyReLU(0.2, True)\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        style = self.to_style(w)\n",
    "        x = self.conv(x, style)\n",
    "        return self.activation(x + self.bias[None, :, None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dWeightModulate(nn.Module):\n",
    "    def __init__(self, in_features, out_features, kernel_size, demodulate=True, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.demodulate = demodulate\n",
    "        self.padding = (kernel_size - 1) // 2\n",
    "\n",
    "        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, s):\n",
    "        b, _, h, w = x.shape\n",
    "        s = s[:, None, :, None, None]\n",
    "        weights = self.weight()[None, :, :, :, :]\n",
    "        weights = weights * s\n",
    "        if self.demodulate:\n",
    "            sigma_inv = torch.rsqrt((weights ** 2).sum(dim=(2, 3, 4), keepdim=True) + self.eps)\n",
    "            weights = weights * sigma_inv\n",
    "        x = x.reshape(1, -1, h, w)\n",
    "        _, _, *ws = weights.shape\n",
    "        weights = weights.reshape(b * self.out_features, *ws)\n",
    "        x = F.conv2d(x, weights, padding=self.padding, groups=b)\n",
    "        return x.reshape(-1, self.out_features, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, log_resolution, n_features = 64, max_features = 256):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        features = [min(max_features, n_features * (2 ** i)) for i in range(log_resolution - 1)]\n",
    "\n",
    "        self.from_rgb = nn.Sequential(\n",
    "            EqualizedConv2d(3, n_features, 1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        n_blocks = len(features) - 1\n",
    "        blocks = [DiscriminatorBlock(features[i], features[i + 1]) for i in range(n_blocks)]\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "        final_features = features[-1] + 1\n",
    "        self.conv = EqualizedConv2d(final_features, final_features, 3)\n",
    "        self.final = EqualizedLinear(2 * 2 * final_features, 1)\n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "        batch_statistics = (\n",
    "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        )\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.from_rgb(x)\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        x = self.minibatch_std(x)\n",
    "        x = self.conv(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.residual = nn.Sequential(nn.AvgPool2d(kernel_size=2, stride=2), # down sampling using avg pool\n",
    "                                      EqualizedConv2d(in_features, out_features, kernel_size=1))\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            EqualizedConv2d(in_features, in_features, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            EqualizedConv2d(in_features, out_features, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "\n",
    "        self.down_sample = nn.AvgPool2d(\n",
    "            kernel_size=2, stride=2\n",
    "        )  # down sampling using avg pool\n",
    "\n",
    "        self.scale = 1 / sqrt(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual(x)\n",
    "\n",
    "        x = self.block(x)\n",
    "        x = self.down_sample(x)\n",
    "\n",
    "        return (x + residual) * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualizedLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias = 0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.weight = EqualizedWeight([out_features, in_features])\n",
    "        self.bias = nn.Parameter(torch.ones(out_features) * bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return F.linear(x, self.weight(), bias=self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualizedConv2d(nn.Module):\n",
    "    def __init__(self, in_features, out_features, kernel_size, padding=0):\n",
    "        super().__init__()\n",
    "        self.padding = padding\n",
    "        self.weight = EqualizedWeight([out_features, in_features, kernel_size, kernel_size])\n",
    "        self.bias = nn.Parameter(torch.ones(out_features))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return F.conv2d(x, self.weight(), bias=self.bias, padding=self.padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualizedWeight(nn.Module):\n",
    "\n",
    "    def __init__(self, shape):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.c = 1 / sqrt(np.prod(shape[1:]))\n",
    "        self.weight = nn.Parameter(torch.randn(shape))\n",
    "\n",
    "    def forward(self):\n",
    "        return self.weight * self.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathLengthPenalty(nn.Module):\n",
    "\n",
    "    def __init__(self, beta):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.beta = beta\n",
    "        self.steps = nn.Parameter(torch.tensor(0.), requires_grad=False)\n",
    "\n",
    "        self.exp_sum_a = nn.Parameter(torch.tensor(0.), requires_grad=False)\n",
    "\n",
    "    def forward(self, w, x):\n",
    "\n",
    "        device = x.device\n",
    "        image_size = x.shape[2] * x.shape[3]\n",
    "        y = torch.randn(x.shape, device=device)\n",
    "\n",
    "        output = (x * y).sum() / sqrt(image_size)\n",
    "        sqrt(image_size)\n",
    "\n",
    "        gradients, *_ = torch.autograd.grad(outputs=output,\n",
    "                                            inputs=w,\n",
    "                                            grad_outputs=torch.ones(output.shape, device=device),\n",
    "                                            create_graph=True)\n",
    "\n",
    "        norm = (gradients ** 2).sum(dim=2).mean(dim=1).sqrt()\n",
    "\n",
    "        if self.steps > 0:\n",
    "\n",
    "            a = self.exp_sum_a / (1 - self.beta ** self.steps)\n",
    "\n",
    "            loss = torch.mean((norm - a) ** 2)\n",
    "        else:\n",
    "            loss = norm.new_tensor(0)\n",
    "\n",
    "        mean = norm.mean().detach()\n",
    "        self.exp_sum_a.mul_(self.beta).add_(mean, alpha=1 - self.beta)\n",
    "        self.steps.add_(1.)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake,device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images)\n",
    " \n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w(batch_size):\n",
    "\n",
    "    z = torch.randn(batch_size, W_DIM).to(DEVICE)\n",
    "    w = mapping_network(z)\n",
    "    return w[None, :, :].expand(LOG_RESOLUTION, -1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(batch_size):\n",
    "    \n",
    "        noise = []\n",
    "        resolution = 4\n",
    "\n",
    "        for i in range(LOG_RESOLUTION):\n",
    "            if i == 0:\n",
    "                n1 = None\n",
    "            else:\n",
    "                n1 = torch.randn(batch_size, 1, resolution, resolution, device=DEVICE)\n",
    "            n2 = torch.randn(batch_size, 1, resolution, resolution, device=DEVICE)\n",
    "\n",
    "            noise.append((n1, n2))\n",
    "\n",
    "            resolution *= 2\n",
    "\n",
    "        return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(gen, epoch, n=100):\n",
    "    \n",
    "    gen.eval()\n",
    "    alpha = 1.0\n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            w     = get_w(1)\n",
    "            noise = get_noise(1)\n",
    "            img = gen(w, noise)\n",
    "            if not os.path.exists(f'saved_examples/epoch{epoch}'):\n",
    "                os.makedirs(f'saved_examples/epoch{epoch}')\n",
    "            save_image(img*0.5+0.5, f\"saved_examples/epoch{epoch}/img_{i}.png\")\n",
    "\n",
    "    gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    critic,\n",
    "    gen,\n",
    "    path_length_penalty,\n",
    "    loader,\n",
    "    opt_critic,\n",
    "    opt_gen,\n",
    "    opt_mapping_network,\n",
    "):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for batch_idx, (real, _) in enumerate(loop):\n",
    "        real = real.to(DEVICE)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        w     = get_w(cur_batch_size)\n",
    "        noise = get_noise(cur_batch_size)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake = gen(w, noise)\n",
    "            critic_fake = critic(fake.detach())\n",
    "            \n",
    "            critic_real = critic(real)\n",
    "            gp = gradient_penalty(critic, real, fake, device=DEVICE)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "                + LAMBDA_GP * gp\n",
    "                + (0.001 * torch.mean(critic_real ** 2))\n",
    "            )\n",
    "\n",
    "        critic.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        opt_critic.step()\n",
    "\n",
    "        gen_fake = critic(fake)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "\n",
    "        if batch_idx % 16 == 0:\n",
    "            plp = path_length_penalty(w, fake)\n",
    "            if not torch.isnan(plp):\n",
    "                loss_gen = loss_gen + plp\n",
    "\n",
    "        mapping_network.zero_grad()\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        opt_mapping_network.step()\n",
    "\n",
    "        loop.set_postfix(\n",
    "            gp=gp.item(),\n",
    "            loss_critic=loss_critic.item(),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MappingNetwork(\n",
       "  (mapping): Sequential(\n",
       "    (0): EqualizedLinear(\n",
       "      (weight): EqualizedWeight()\n",
       "    )\n",
       "    (1): ReLU()\n",
       "    (2): EqualizedLinear(\n",
       "      (weight): EqualizedWeight()\n",
       "    )\n",
       "    (3): ReLU()\n",
       "    (4): EqualizedLinear(\n",
       "      (weight): EqualizedWeight()\n",
       "    )\n",
       "    (5): ReLU()\n",
       "    (6): EqualizedLinear(\n",
       "      (weight): EqualizedWeight()\n",
       "    )\n",
       "    (7): ReLU()\n",
       "    (8): EqualizedLinear(\n",
       "      (weight): EqualizedWeight()\n",
       "    )\n",
       "    (9): ReLU()\n",
       "    (10): EqualizedLinear(\n",
       "      (weight): EqualizedWeight()\n",
       "    )\n",
       "    (11): ReLU()\n",
       "    (12): EqualizedLinear(\n",
       "      (weight): EqualizedWeight()\n",
       "    )\n",
       "    (13): ReLU()\n",
       "    (14): EqualizedLinear(\n",
       "      (weight): EqualizedWeight()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader              = get_loader()\n",
    "\n",
    "gen                 = Generator(LOG_RESOLUTION, W_DIM).to(DEVICE)\n",
    "critic              = Discriminator(LOG_RESOLUTION).to(DEVICE)\n",
    "mapping_network     = MappingNetwork(Z_DIM, W_DIM).to(DEVICE)\n",
    "path_length_penalty = PathLengthPenalty(0.99).to(DEVICE)\n",
    "\n",
    "opt_gen             = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
    "opt_critic          = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
    "opt_mapping_network = optim.Adam(mapping_network.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
    "\n",
    "gen.train()\n",
    "critic.train()\n",
    "mapping_network.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/673 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/673 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=32, weight of size [8192, 256, 3, 3], expected input[1, 32, 4, 4] to have 8192 channels, but got 32 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb Cell 23\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m loader \u001b[39m=\u001b[39m get_loader()  \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     train_fn(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         critic,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         gen,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         path_length_penalty,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         loader,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         opt_critic,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         opt_gen,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         opt_mapping_network,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \tgenerate_examples(gen, epoch)\n",
      "\u001b[1;32m/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m noise \u001b[39m=\u001b[39m get_noise(cur_batch_size)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast():\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     fake \u001b[39m=\u001b[39m gen(w, noise)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     critic_fake \u001b[39m=\u001b[39m critic(fake\u001b[39m.\u001b[39mdetach())\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     critic_real \u001b[39m=\u001b[39m critic(real)\n",
      "File \u001b[0;32m~/miniconda3/envs/stylegan/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m batch_size \u001b[39m=\u001b[39m w\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial_constant\u001b[39m.\u001b[39mexpand(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstyle_block(x, w[\u001b[39m0\u001b[39;49m], input_noise[\u001b[39m0\u001b[39;49m][\u001b[39m1\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m rgb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_rgb(x, w[\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_blocks):\n",
      "File \u001b[0;32m~/miniconda3/envs/stylegan/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, w, noise):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_style(w)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x, s)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mif\u001b[39;00m noise \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_noise[\u001b[39mNone\u001b[39;00m, :, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m noise\n",
      "File \u001b[0;32m~/miniconda3/envs/stylegan/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m _, _, \u001b[39m*\u001b[39mws \u001b[39m=\u001b[39m weights\u001b[39m.\u001b[39mshape\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m weights \u001b[39m=\u001b[39m weights\u001b[39m.\u001b[39mreshape(b \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features, \u001b[39m*\u001b[39mws)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mconv2d(x, weights, padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, groups\u001b[39m=\u001b[39;49mb)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Adam/Documents/PatternAnalysis-2023/recognition/ADNI-Style-GAN2-s4696807/practice_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features, h, w)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=32, weight of size [8192, 256, 3, 3], expected input[1, 32, 4, 4] to have 8192 channels, but got 32 channels instead"
     ]
    }
   ],
   "source": [
    "loader = get_loader()  \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_fn(\n",
    "        critic,\n",
    "        gen,\n",
    "        path_length_penalty,\n",
    "        loader,\n",
    "        opt_critic,\n",
    "        opt_gen,\n",
    "        opt_mapping_network,\n",
    "    )\n",
    "    if epoch % 50 == 0:\n",
    "    \tgenerate_examples(gen, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
